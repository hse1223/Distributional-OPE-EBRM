Order of running codes
1.py: learn optimal policy 
- We recommend not to run 1.py, since it will alter the learned model.
- This contains randomness, which we did not control with seed.
2.py: obtain MC return approximation (marginal).
- Manually select the game (acrobot, cartpole, mountaincar).
3-1.py: QRDQN simulations.
- Manually select the game (acrobot, cartpole, mountaincar).
3-2.py: MMDQN simulations. 
- Manually select the game (acrobot, cartpole, mountaincar).
3-3.py: EBRM simulations. 
- Manually select the game (acrobot, cartpole, mountaincar).
4.py
- Compares the simulations.

About env.seed() before env.reset()
- Sampling from inital state distribuiton: Put env.seed(seed) in front.
- Reaching the terminal state: Put env.seed(1) so that it proceeds to the fixed state.